From 9124554c09c3ff4e217bd5c81f46f0da6da44462 Mon Sep 17 00:00:00 2001
From: Michael Matz <matz@suse.de>
Date: Thu, 4 Apr 2013 16:59:54 +0200
Subject: [PATCH 139/169] Fix CPU unpinning

When there are bits set in the mask outside what the kernels
size is glibc throws EINVAL despite the fact that there are
also bits set below that border.  Manpage says EINVAL should
only be given when there are _no_ possible bits set.  Well,
too bad, deal with it, just remember the old cpu mask.
---
 linux-user/main.c    | 42 ++++++++++++++++++++++++++++++++++++++++++
 linux-user/qemu.h    |  2 ++
 linux-user/signal.c  | 32 ++------------------------------
 linux-user/syscall.c | 42 +++---------------------------------------
 4 files changed, 49 insertions(+), 69 deletions(-)

Index: qemu-1.6.0+dfsg/linux-user/main.c
===================================================================
--- qemu-1.6.0+dfsg.orig/linux-user/main.c	2013-11-05 22:23:44.375361082 +0000
+++ qemu-1.6.0+dfsg/linux-user/main.c	2013-11-05 22:23:44.367361102 +0000
@@ -207,6 +207,48 @@
     pthread_mutex_unlock(&cpu_list_mutex);
 }
 
+static int pinned_cpu = -1;
+static cpu_set_t old_cpu_mask;
+
+void pin_to_one_cpu (void)
+{
+  cpu_set_t mask;
+  if (pinned_cpu < 0) {
+      /* Try to find out a randomized CPU to which to pin,
+	 so that potential parallel qemu processes don't
+	 compete for the same CPU.  Select one of the
+	 CPUs to which we're pinned already by counting
+	 them and choosing the "random"th one.  */
+      int count, i;
+      if (sched_getaffinity (0, sizeof(old_cpu_mask), &old_cpu_mask) < 0)
+	pinned_cpu = 0;
+      else {
+	  count = CPU_COUNT (&old_cpu_mask);
+	  if (count > 1) {
+	      pinned_cpu = getpid() % count;
+	      for (i = 0; i < CPU_SETSIZE; i++)
+		if (CPU_ISSET (i, &old_cpu_mask)) {
+		    if (pinned_cpu-- == 0)
+		      break;
+		}
+	      pinned_cpu = i;
+	  } else
+	    pinned_cpu = 0;
+      }
+  }
+  CPU_ZERO(&mask);
+  CPU_SET(pinned_cpu, &mask);
+  sched_setaffinity(0, sizeof(mask), &mask);
+}
+
+void unpin (void)
+{
+  if (pinned_cpu < 0)
+    return;
+  pinned_cpu = -1;
+  if (sched_setaffinity(0, sizeof(old_cpu_mask), &old_cpu_mask) < 0)
+    perror ("XXXXX Huh?");
+}
 
 #ifdef TARGET_I386
 /***********************************************************/
Index: qemu-1.6.0+dfsg/linux-user/qemu.h
===================================================================
--- qemu-1.6.0+dfsg.orig/linux-user/qemu.h	2013-11-05 22:23:44.375361082 +0000
+++ qemu-1.6.0+dfsg/linux-user/qemu.h	2013-11-05 22:23:44.367361102 +0000
@@ -271,6 +271,8 @@
 
 /* main.c */
 extern unsigned long guest_stack_size;
+extern void pin_to_one_cpu (void);
+extern void unpin (void);
 
 /* user access */
 
Index: qemu-1.6.0+dfsg/linux-user/signal.c
===================================================================
--- qemu-1.6.0+dfsg.orig/linux-user/signal.c	2013-11-05 22:23:44.375361082 +0000
+++ qemu-1.6.0+dfsg/linux-user/signal.c	2013-11-05 22:23:44.371361092 +0000
@@ -652,36 +652,8 @@
              * pin our guest process to a single host CPU if we're using the
              * boehm-gc.
              */
-            if ((k->sa_flags & TARGET_SA_RESTART) && host_sig == SIGPWR) {
-		static int thecpu = -1;
-                cpu_set_t mask;
-		if (thecpu < 0) {
-		    /* Try to find out a randomized CPU to which to pin,
-		       so that potential parallel qemu processes don't
-		       compete for the same CPU.  Select one of the
-		       CPUs to which we're pinned already by counting
-		       them and choosing the "random"th one.  */
-		    int count, i;
-		    if (sched_getaffinity (0, sizeof(mask), &mask) < 0)
-		        thecpu = 0;
-		    else {
-			count = CPU_COUNT (&mask);
-			if (count > 1) {
-			    thecpu = getpid() % count;
-			    for (i = 0; i < CPU_SETSIZE; i++)
-			        if (CPU_ISSET (i, &mask)) {
-				    if (thecpu-- == 0)
-				        break;
-			        }
-			    thecpu = i;
-			} else
-			  thecpu = 0;
-		    }
-		}
-                CPU_ZERO(&mask);
-                CPU_SET(thecpu, &mask);
-                sched_setaffinity(0, sizeof(mask), &mask);
-            }
+            if ((k->sa_flags & TARGET_SA_RESTART) && host_sig == SIGPWR)
+		pin_to_one_cpu ();
 #else
             if (k->sa_flags & TARGET_SA_RESTART)
                 act1.sa_flags |= SA_RESTART;
Index: qemu-1.6.0+dfsg/linux-user/syscall.c
===================================================================
--- qemu-1.6.0+dfsg.orig/linux-user/syscall.c	2013-11-05 22:23:44.375361082 +0000
+++ qemu-1.6.0+dfsg/linux-user/syscall.c	2013-11-05 22:23:44.371361092 +0000
@@ -4338,36 +4338,7 @@
 
         /* agraf: Pin ourselves to a single CPU when running multi-threaded.
            This turned out to improve stability for me. */
-        {
-	    static int thecpu = -1;
-	    cpu_set_t mask;
-	    if (thecpu < 0) {
-		/* Try to find out a randomized CPU to which to pin,
-		   so that potential parallel qemu processes don't
-		   compete for the same CPU.  Select one of the
-		   CPUs to which we're pinned already by counting
-		   them and choosing the "random"th one.  */
-		int count, i;
-		if (sched_getaffinity (0, sizeof(mask), &mask) < 0)
-		    thecpu = 0;
-		else {
-		    count = CPU_COUNT (&mask);
-		    if (count > 1) {
-			thecpu = getpid() % count;
-			for (i = 0; i < CPU_SETSIZE; i++)
-			    if (CPU_ISSET (i, &mask)) {
-			        if (thecpu-- == 0)
-				    break;
-			    }
-			thecpu = i;
-		    } else
-		        thecpu = 0;
-		}
-	    }
-	    CPU_ZERO(&mask);
-	    CPU_SET(thecpu, &mask);
-	    sched_setaffinity(0, sizeof(mask), &mask);
-        }
+	pin_to_one_cpu ();
 
         /* Grab a mutex so that thread setup appears atomic.  */
         pthread_mutex_lock(&clone_lock);
@@ -4433,15 +4404,8 @@
                 cpu_set_tls (env, newtls);
             if (flags & CLONE_CHILD_CLEARTID)
                 ts->child_tidptr = child_tidptr;
-	    {
-	        /* Activate all CPUs again when forking.  */
-	        int i;
-		cpu_set_t mask;
-		CPU_ZERO(&mask);
-		for (i = 0; i < CPU_SETSIZE; i++)
-		  CPU_SET(i, &mask);
-		sched_setaffinity(0, sizeof(mask), &mask);
-	    }
+	    /* Activate all CPUs again when forking.  */
+	    unpin ();
         } else {
             fork_end(0);
         }
